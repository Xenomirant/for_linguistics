re.findall(pattern, emails, flags=re.IGNORECASE) 

re.findall(r'<p>(.*)</p>', text)

mail = r'.*@\w+\.\w+'

print(re.findall(r'\w+', '443031 Самара'))
Output: ['443031', 'Самара']
____________________________________________________________________
# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))
____________________________________________________________________
TOKENIZATION

from nltk.tokenize import sent_tokenize 
from nltk.tokenize import word_tokenize 

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))
____________________________________________________________________
from nltk.tokenize import regexp_tokenize, TweetTokenizer 
# Write a pattern that matches both mentions (@) and hashtags
pattern2 = r"([@|#]\w+)"
# Use the pattern on the last tweet in the tweets list
mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)
print(mentions_hashtags)
____________________________________________________________________
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer
# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)
____________________________________________________________________
all_words = word_tokenize(german_text)
print(all_words)

capital_words = r"[A-ZÜ]\w+"
print(regexp_tokenize(german_text, capital_words))

emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))
____________________________________________________________________
# list comprehension
word_lengths = [len(w) for w in words]

from matplotlib import pyplot as plt
from nltk.tokenize import word_tokenize

# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s, r"\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]
plt.hist(line_num_words)
plt.show()
____________________________________________________________________
BAG-OF-WORDS

from collections import Counter
tokens = word_tokenize(article)
lower_tokens = [t.lower() for t in tokens]
                  
# Create a Counter with the lowercase tokens
bow_simple = Counter(lower_tokens)
# Print the 10 most common tokens
print(bow_simple.most_common(10))
____________________________________________________________________
# To remove non-alphabetic
tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]
# To remove stopwords for English
no_stops = [t for t in tokens if t not in stopwords.words('english')]
print(Counter(no_stops).most_common(2))
____________________________________________________________________
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]
# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)
# Print the 10 most common tokens
print(bow.most_common(10))
____________________________________________________________________
